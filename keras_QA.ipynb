{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"keras_QA.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1OtW8gVZ5u20bOEZdsx2GP121AMtd-KNo","authorship_tag":"ABX9TyPij0ZO9/6Ns1gFfthJaC4/"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jjdgjGo0Wu2G","executionInfo":{"status":"ok","timestamp":1610335691632,"user_tz":-480,"elapsed":2138,"user":{"displayName":"Tao Guan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4-6vJCQVkP7tPscmwH-TCw9oP8Z0062aElcseQA=s64","userId":"14923390469277448374"}},"outputId":"f9cd4658-8b97-47f4-84b1-2e01f29be183"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mon Jan 11 03:28:10 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cOaDX2nur6RL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610335695897,"user_tz":-480,"elapsed":1877,"user":{"displayName":"Tao Guan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4-6vJCQVkP7tPscmwH-TCw9oP8Z0062aElcseQA=s64","userId":"14923390469277448374"}},"outputId":"e50acbb5-69f8-4520-cd2d-8e06d6f38e24"},"source":["%tensorflow_version 1.x"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fEnNgKkACYms","executionInfo":{"status":"ok","timestamp":1610335708507,"user_tz":-480,"elapsed":9055,"user":{"displayName":"Tao Guan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4-6vJCQVkP7tPscmwH-TCw9oP8Z0062aElcseQA=s64","userId":"14923390469277448374"}},"outputId":"253a327a-750b-4fcb-a70d-677a6934fc6f"},"source":["!pip3 install nltk\n","import nltk\n","nltk.download('punkt')\n","\n","!pip3 install toposort"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Collecting toposort\n","  Downloading https://files.pythonhosted.org/packages/f2/7d/55784e894ee0cde2474fb977ffd1651e74e840a9f92e1d847f7e3115d5ec/toposort-1.6-py2.py3-none-any.whl\n","Installing collected packages: toposort\n","Successfully installed toposort-1.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lBgZQn_u43-k"},"source":["#import os\n","#import urllib.request\n","#urllib.request.urlretrieve('https://storage.googleapis.com/albert_models/albert_base_zh.tar.gz','albert_base_zh.tar.gz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yivAfdrh9DoK","executionInfo":{"status":"ok","timestamp":1610335713097,"user_tz":-480,"elapsed":1525,"user":{"displayName":"Tao Guan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4-6vJCQVkP7tPscmwH-TCw9oP8Z0062aElcseQA=s64","userId":"14923390469277448374"}},"outputId":"c1772053-8e92-48dd-e577-7b86f4b34be7"},"source":["import sys\n","sys.path.append('/content/drive/MyDrive/colab_data/packages/albert_QA_colab')\n","#sys.path=['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython']\n","print(sys.path)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['/tensorflow-1.15.2/python3.6', '', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/MyDrive/colab_data/packages/albert_QA_colab']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBPMphH6rX44","executionInfo":{"status":"ok","timestamp":1610343948437,"user_tz":-480,"elapsed":2941,"user":{"displayName":"Tao Guan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4-6vJCQVkP7tPscmwH-TCw9oP8Z0062aElcseQA=s64","userId":"14923390469277448374"}},"outputId":"b67480e5-8692-48eb-f1b0-d46aa216abe4"},"source":["import os\n","\n","# AMP要使用 tf.keras \n","os.environ[\"TF_KERAS\"] = \"1\"\n","# 重计算设置\n","import memory_saving_gradients as gc\n","from tensorflow.python.ops import gradients as tf_gradients\n","tf_gradients.gradients = gc.gradients_collection\n","\n","import json, shutil\n","import numpy as np\n","from bert4keras.backend import keras, K, tf\n","from bert4keras.models import build_transformer_model\n","from bert4keras.tokenizers import Tokenizer\n","from bert4keras.optimizers import Adam\n","from bert4keras.snippets import sequence_padding, DataGenerator\n","from bert4keras.snippets import open\n","from keras.layers import Layer, Dense, Permute\n","from keras.models import Model\n","from tqdm import tqdm\n","\n","print(tf.__version__)\n","print(tf.test.is_gpu_available())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["1.15.2\n","True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M634phImDISK"},"source":["# 基本信息\n","maxlen = 512\n","epochs = 10\n","batch_size = 64\n","learing_rate = 2e-5\n","\n","\n","# 百度 阅读理解\n","train_data_file1 = '/content/drive/MyDrive/colab_data/data/dureader_robust-data/train.json'  \n","eva_data_file = '/content/drive/MyDrive/colab_data/data/dureader_robust-data/dev.json'\n","eva_script = '/content/drive/MyDrive/colab_data/packages/albert_QA_colab/evaluate/evaluate_dureader.py'\n","\n","# CMRC2018\n","train_data_file2 = '/content/drive/MyDrive/colab_data/data/cmrc2018/cmrc2018_train_dev.json'\n","#eva_data_file = '/content/drive/MyDrive/colab_data/data/cmrc2018/cmrc2018_trial.json'\n","#eva_script = '/content/drive/MyDrive/colab_data/packages/albert_QA_colab/evaluate/evaluate_cmrc2018.py'\n","\n","'''\n","bert4keras 支持的 BERT model_type\n","    'bert': BERT,\n","    'albert': ALBERT,\n","    'albert_unshared': ALBERT_Unshared,\n","    'roberta': BERT,\n","'''\n","\n","# bert配置\n","#model_type = 'bert'\n","#config_path = '/content/drive/MyDrive/colab_data/models/bert_chinese_L-12_H-768_A-12/bert_config.json'\n","#checkpoint_path = '/content/drive/MyDrive/colab_data/models/bert_chinese_L-12_H-768_A-12/bert_model.ckpt'\n","#dict_path = '/content/drive/MyDrive/colab_data/models/bert_chinese_L-12_H-768_A-12/vocab.txt'\n","\n","# albert配置\n","model_type = 'albert'\n","config_path = '/content/drive/MyDrive/colab_data/models/albert_base/albert_config.json'\n","checkpoint_path = '/content/drive/MyDrive/colab_data/models/albert_base/model.ckpt-best'\n","dict_path = '/content/drive/MyDrive/colab_data/models/albert_base/vocab_chinese.txt'\n","\n","# 输出目录\n","output_path = '/content/drive/MyDrive/colab_data/outputs/keras_QA'\n","output_path = '%s/batch%d_max%d_lr%.0e'%(output_path, batch_size, maxlen, learing_rate)\n","\n","\n","# 兼容两个数据集的载入\n","def load_data(filename_list):\n","    D = []\n","    for filename in filename_list:\n","        for d in json.load(open(filename))['data']:\n","            for pp in d['paragraphs']:\n","                for qa in pp['qas']:\n","                    D.append([\n","                        qa['id'], pp['context'], qa['question'],\n","                        [a['text'] for a in qa.get('answers', [])]\n","                    ])\n","    return D\n","\n","\n","# 读取数据, 使用两个数据集一起训练\n","train_data = load_data([train_data_file1, train_data_file2])\n","\n","\n","# 建立分词器\n","tokenizer = Tokenizer(dict_path, do_lower_case=True)\n","\n","\n","def search(pattern, sequence):\n","    \"\"\"从sequence中寻找子串pattern\n","    如果找到，返回第一个下标；否则返回-1。\n","    \"\"\"\n","    n = len(pattern)\n","    for i in range(len(sequence)):\n","        if sequence[i:i + n] == pattern:\n","            return i\n","    return -1\n","\n","\n","class data_generator(DataGenerator):\n","    \"\"\"数据生成器\n","    \"\"\"\n","    def __iter__(self, random=False):\n","        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n","        for is_end, item in self.sample(random):\n","            context, question, answers = item[1:]\n","            token_ids, segment_ids = tokenizer.encode(\n","                question, context, maxlen=maxlen\n","            )\n","            a = np.random.choice(answers)\n","            a_token_ids = tokenizer.encode(a)[0][1:-1]\n","            start_index = search(a_token_ids, token_ids)\n","            if start_index != -1:\n","                labels = [[start_index], [start_index + len(a_token_ids) - 1]]\n","                batch_token_ids.append(token_ids)\n","                batch_segment_ids.append(segment_ids)\n","                batch_labels.append(labels)\n","                if len(batch_token_ids) == self.batch_size or is_end:\n","                    batch_token_ids = sequence_padding(batch_token_ids)\n","                    batch_segment_ids = sequence_padding(batch_segment_ids)\n","                    batch_labels = sequence_padding(batch_labels)\n","                    yield [batch_token_ids, batch_segment_ids], batch_labels\n","                    batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n","\n","\n","# 优化器，使用AMP\n","opt = Adam(learing_rate)\n","opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n","\n","# 建立模型，载入权重\n","class MaskedSoftmax(Layer):\n","    \"\"\"在序列长度那一维进行softmax，并mask掉padding部分\n","    \"\"\"\n","    def compute_mask(self, inputs, mask=None):\n","        return None\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = K.cast(mask, K.floatx())\n","            mask = K.expand_dims(mask, 2)\n","            inputs = inputs - (1.0 - mask) * 1e12\n","        return K.softmax(inputs, 1)\n","\n","\n","model = build_transformer_model(\n","    config_path=config_path,\n","    checkpoint_path=checkpoint_path,\n","    model=model_type\n",")\n","\n","\n","output = Dense(2)(model.output)\n","output = MaskedSoftmax()(output)\n","output = Permute((2, 1))(output)\n","\n","model = Model(model.input, output)\n","model.summary()\n","\n","\n","def sparse_categorical_crossentropy(y_true, y_pred):\n","    # y_true需要重新明确一下shape和dtype\n","    y_true = K.reshape(y_true, K.shape(y_pred)[:-1])\n","    y_true = K.cast(y_true, 'int32')\n","    y_true = K.one_hot(y_true, K.shape(y_pred)[2])\n","    # 计算交叉熵\n","    return K.mean(K.categorical_crossentropy(y_true, y_pred))\n","\n","\n","def sparse_accuracy(y_true, y_pred):\n","    # y_true需要重新明确一下shape和dtype\n","    y_true = K.reshape(y_true, K.shape(y_pred)[:-1])\n","    y_true = K.cast(y_true, 'int32')\n","    # 计算准确率\n","    y_pred = K.cast(K.argmax(y_pred, axis=2), 'int32')\n","    return K.mean(K.cast(K.equal(y_true, y_pred), K.floatx()))\n","\n","\n","model.compile(\n","    loss=sparse_categorical_crossentropy,\n","    optimizer=opt,\n","    metrics=[sparse_accuracy]\n",")\n","\n","\n","def extract_answer(question, context, max_a_len=16):\n","    \"\"\"抽取答案函数\n","    \"\"\"\n","    max_q_len = 64\n","    q_token_ids = tokenizer.encode(question, maxlen=max_q_len)[0]\n","    c_token_ids = tokenizer.encode(\n","        context, maxlen=maxlen - len(q_token_ids) + 1\n","    )[0]\n","    token_ids = q_token_ids + c_token_ids[1:]\n","    segment_ids = [0] * len(q_token_ids) + [1] * (len(c_token_ids) - 1)\n","    c_tokens = tokenizer.tokenize(context)[1:-1]\n","    mapping = tokenizer.rematch(context, c_tokens)\n","    probas = model.predict([[token_ids], [segment_ids]])[0]\n","    probas = probas[:, len(q_token_ids):-1]\n","    start_end, score = None, -1\n","    for start, p_start in enumerate(probas[0]):\n","        for end, p_end in enumerate(probas[1]):\n","            if end >= start and end < start + max_a_len:\n","                if p_start * p_end > score:\n","                    start_end = (start, end)\n","                    score = p_start * p_end\n","    start, end = start_end\n","    return context[mapping[start][0]:mapping[end][-1] + 1]\n","\n","\n","def predict_to_file(infile, out_file):\n","    \"\"\"预测结果到文件，方便提交\n","    \"\"\"\n","    fw = open(out_file, 'w', encoding='utf-8')\n","    R = {}\n","    for d in tqdm(load_data([infile])):\n","        a = extract_answer(d[2], d[1])\n","        R[d[0]] = a\n","    R = json.dumps(R, ensure_ascii=False, indent=4)\n","    fw.write(R)\n","    fw.close()\n","\n","\n","def evaluate(filename):\n","    \"\"\"评测函数（官方提供评测脚本evaluate.py）\n","    \"\"\"\n","    predict_to_file(filename, 'evaluate.pred.json')\n","    metrics = json.loads(\n","        os.popen(\n","            'python3 %s %s %s'\n","            % (eva_script, filename, 'evaluate.pred.json')\n","        ).read().strip()\n","    )\n","    return metrics\n","\n","\n","class Evaluator(keras.callbacks.Callback):\n","    \"\"\"评估和保存模型\n","    \"\"\"\n","    def __init__(self):\n","        self.best_val_f1 = 0.\n","        self.last_best_path = ''\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        metrics = evaluate(eva_data_file)\n","        if float(metrics['F1']) >= self.best_val_f1:\n","            self.best_val_f1 = float(metrics['F1'])\n","            self.save_weights(metrics['F1'])\n","        metrics['BEST F1'] = self.best_val_f1\n","        print(metrics)\n","\n","    def save_weights(self, f1):\n","        if os.path.exists(self.last_best_path):\n","            shutil.rmtree(self.last_best_path)\n","        best_path = '%s_F1_%s'%(output_path, f1)\n","        os.makedirs(best_path, exist_ok=True)\n","        model.save_weights('%s/best_model.weights' % best_path)\n","        self.last_best_path = best_path\n","\n","\n","# main()\n","print(\"maxlen: \", maxlen)\n","print(\"epochs: \", epochs)\n","print(\"batch_size: \", batch_size)\n","print(\"learing_rate: \", learing_rate)\n","print(\"train data: \", len(train_data))\n","\n","train_generator = data_generator(train_data, batch_size)\n","evaluator = Evaluator()\n","\n","model.fit_generator(\n","    train_generator.forfit(),\n","    steps_per_epoch=len(train_generator),\n","    epochs=epochs,\n","    callbacks=[evaluator]\n",")"],"execution_count":null,"outputs":[]}]}